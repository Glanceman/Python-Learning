{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** \\_\\_\\_\\_\\_\n",
    "\n",
    "**EID:** \\_\\_\\_\\_\\_\n",
    "\n",
    "**Kaggle Team Name:** \\_\\_\\_\\_\\_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5489 - Assignment 2 - Urban Sound Tagging\n",
    "Due date: see Assignment 2 on Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "In this assignment, the task is to annotate or tag a sound clip with descriptive (semantic) keywords.  This kind of content-based tagging system could be useful to musicians and sound engineers who want to automatically organize their sound library, or search for sound or music by keyword.\n",
    "\n",
    "## Dataset of Urban Sounds\n",
    "\n",
    "This dataset contains urban sounds recorded on various sensors around a busy city.  Each sound clip is annotated with semantic tags. _The data is publicly available - please do not search for it and cheat._\n",
    "\n",
    "\n",
    "## Methodology\n",
    "Semantic annotation is a multi-label classification problem, where each label corresponds to one sound tag and is a binary classification problem. The labels can co-occur (multiple labels can be assigned to the same sound), which makes it different from multi-class classification (where only one label can be assigned).  Sound is a temporal process, so the important thing is how to define the _feature space_ for representing the sound, before learning the binary classifiers. You are free to choose appropriate methods (e.g., feature extraction method, dimensionality reduction, and clustering methods) to help define a suitable feature space for sound annotation.  You are free to use methods that were not introduced in class, as long as you present the details in the report.  You can also consider the co-occurence of the labels to help with the multi-label classification.\n",
    "\n",
    "\n",
    "## Evaluation of Tagging\n",
    "\n",
    "For evaluation, you will predict the presence/absence of tags for each test sound. The evaluation metric is \"Mean column-wise AUC\".  AUC is the area under the ROC curve, which plots FPR vs TPR.  \"Mean column-wise\" computes the average of the AUCs for the tags.  To compute AUC, you will need to predict the score of each label (e.g., decision function value, probability, etc.) rather than the label.\n",
    "\n",
    "\n",
    "## Evaluation on Kaggle\n",
    "\n",
    "You need to submit your test predictions to Kaggle for evaluation.  50% of the test data will be used to show your ranking on the live leaderboard.  After the assignment deadline, the remaining 50% will be used to calculate your final ranking. The entry with the highest final ranking will win a prize!  Also the top-ranked entries will be asked to give a short 5 minute presentation on what they did.\n",
    "\n",
    "To submit to Kaggle you need to create an account, and use the competition invitation that will be posted on Canvas. You must submit your Kaggle account name to the \"Kaggle Username\" assignment on Canvas 1 week before the Assignment 2 deadline. This is to prevent students from creating multiple Kaggle accounts to gain unfair advantage.\n",
    "\n",
    "**Note:** You can only submit 2 times per day to Kaggle!\n",
    "\n",
    "\n",
    "## What to hand in\n",
    "You need to turn in the following things:\n",
    "\n",
    "1. This ipynb file `Assignment2.ipynb` with your source code and documentation.  _**You should write about all the various attempts that you make to find a good solution.**_ You may also submit python scripts as source code, but your documentation must be in the ipynb file.\n",
    "2. Your final csv submission file to Kaggle.\n",
    "3. The ipynb file `Assignment2-Final.ipynb`, which contains the code that generates the final submission file that you submit to Kaggle.  **This code will be used to verify that your Kaggle submission is reproducible.**\n",
    "4. Your Kaggle username (submitted to the \"Kaggle Username\" assignment on Canvas 1 week before the Assignment 2 deadline)\n",
    "\n",
    "Files should be uploaded to Assignment 2 on Canvas.\n",
    "\n",
    "\n",
    "## Grading\n",
    "The marks of the assignment are distributed as follows:\n",
    "- 45% - Results using various feature representations, dimensionality reduction methods, classification methods, etc.\n",
    "- 30% - Trying out feature representations (e.g. adding additional features, combining features from different sources) or methods not used in the tutorials.\n",
    "- 20% - Quality of the written report.  More points for insightful observations and analysis.\n",
    "- 5% - Final ranking on the Kaggle test data (private leaderboard). If a submission cannot be reproduced by the submitted code, it will not receive marks for ranking.\n",
    "- **Late Penalty:** 25 marks will be subtracted for each day late.\n",
    "\n",
    "**Note: This is an individual assignment. Every student must turn in their own work!**\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:50:47.103900Z",
     "start_time": "2023-10-06T02:50:46.380887Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib_inline   # setup output image format\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from scipy import stats\n",
    "random.seed(100)\n",
    "import csv\n",
    "from scipy import io\n",
    "import pickle\n",
    "from IPython.display import Audio, display\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:50:47.125750Z",
     "start_time": "2023-10-06T02:50:47.105060Z"
    }
   },
   "outputs": [],
   "source": [
    "def showAudio(info):\n",
    "    myfile = 'soundwav/' + info['fname'] + '.wav'\n",
    "    if os.path.exists(myfile):\n",
    "        display(Audio(myfile))\n",
    "    else:\n",
    "        print(\"*** wav file \" + myfile + \" could not be found ***\")\n",
    "\n",
    "def load_pickle(fname):\n",
    "    f = open(fname, 'rb')\n",
    "    out = pickle.load(f)\n",
    "    f.close()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "The training and test data are stored in various pickle files. Here we assume the data is stored in the `musicdata` directory. The below code will load the data, including tags and extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:50:47.436899Z",
     "start_time": "2023-10-06T02:50:47.129368Z"
    }
   },
   "outputs": [],
   "source": [
    "train_tags  = load_pickle('sounddata/train_tags.pickle3')\n",
    "train_mfccs = load_pickle('sounddata/train_mfccs.pickle3')\n",
    "train_mels  = load_pickle('sounddata/train_mels.pickle3')\n",
    "train_info  = load_pickle('sounddata/train_info.pickle3')\n",
    "\n",
    "test_mfccs = load_pickle('sounddata/test_mfccs.pickle3')\n",
    "test_mels  = load_pickle('sounddata/test_mels.pickle3')\n",
    "test_info  = load_pickle('sounddata/test_info.pickle3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the things in the dataset:\n",
    "\n",
    "- `train_info` - info about each sound in the training set.\n",
    "- `train_mels` - the Mel-frequency spectrogram for each sound in the training set. Mel-frequency is a logarithmically-transformed frequency with better perceptual distance.  More details [here](https://towardsdatascience.com/learning-from-audio-the-mel-scale-mel-spectrograms-and-mel-frequency-cepstral-coefficients-f5752b6324a8).\n",
    "- `train_mfccs` -  MFCCs (Mel-frequency cepstrum coefficients) are dimensionality-reduced version of the Mel-frequency spectrogram. Specifically, the log is applied to the magnitudes, and then a Discrete Cosine Transform is applied at each time. \n",
    "- `train_tags` - the descriptive tags for each sound in the training set.\n",
    "- `test_info` - info about each sound in the test set.\n",
    "- `test_mels` - the Mel Spectrogram for each sound in the test.\n",
    "- `test_mfccs` - the MFCC features for each sound in the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the one sample in the training set, as well as the tags and other info. To play the audio, we assume the wav files are available in the `soundwav` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:50:47.463064Z",
     "start_time": "2023-10-06T02:50:47.441371Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ii = 160\n",
    "showAudio(train_info[ii])\n",
    "print(\"tags:\", train_tags[ii])\n",
    "print(\"info:\", train_info[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `info` dictionary contains some additional information. Each sound is annotated by multiple people, and the `counts` field has the percentage of people who annotated that the sound was present.  The `tags` were generated from these counts, where a tag is present if at least one person annotated it in the clip (i.e., count>0).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "\n",
    "Here are the tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T03:25:54.822585Z",
     "start_time": "2023-10-06T03:25:54.789055Z"
    }
   },
   "outputs": [],
   "source": [
    "alltagnames = unique(concatenate(train_tags))\n",
    "for a in alltagnames:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags are organized hierarchically. The coarse-level has 8 tags (e.g., `1_engine_presence`), and there are fine-level tags associated with each coarse-level (e.g., `1-1_small-sounding-engine_presence`). \n",
    "\n",
    "The fine-level tags preceded by an `X` code (e.g., `1-X_engine-of-uncertain-size_presence`) indicate that the annotator could identiy the course-level tag, but not the fine-level tag due to uncertainty or because there was no appropriate fine-level tag. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio representation\n",
    "Here is the Mel-frequency spectrogram, which shows the frequency content over time. The spectrogram is stored in an `B x T` matrix, where `B` is the number of bins, and `T` is the temporal length.  The left plot shows the original Mel spectrogram (with time increasing to the right).  The right plot shows the log magnitude, which can better visualize the differences.  Here we use `B=128` Mel-bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:50:47.671743Z",
     "start_time": "2023-10-06T02:50:47.465686Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_mels[ii].shape)\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(train_mels[ii].T);\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('mel bin');\n",
    "plt.title('Mel spectrogram')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(log(train_mels[ii].T));\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('mel bin');\n",
    "plt.title('log Mel spectrogram')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCCs are a dimensionality-reduced version of the Mel-spectrogram.  To get the MFCC, the Discrete Cosine Transform (DCT) is applied to each 128-dim log-Mel bin vector.  Here we use 20-dimension DCT, so the 128-dim vector is convereted to 20-dim in each time step.  The left plot shows the MFCCs as an image, while the right plots the individual dimensions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:50:47.834921Z",
     "start_time": "2023-10-06T02:50:47.673865Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_mfccs[ii].shape)\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(train_mfccs[ii].T)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('mfcc bin')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_mfccs[ii]);\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('mfcc value');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing - Delta MFCCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you might notice is that the MFCC vectors are time-series.  One trick to include time-series information into a vector representation is to append the difference between two consecutive feature vectors.  This way, we can include some relationship between two time steps in the representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:50:47.841000Z",
     "start_time": "2023-10-06T02:50:47.837523Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute delta MFCCs\n",
    "def compute_delta_mfccs(mfccs):\n",
    "    dmfccs = []\n",
    "    for m in mfccs:\n",
    "        tmp = m[1:] - m[0:-1]\n",
    "        dm = hstack((m[0:-1], tmp))\n",
    "        dmfccs.append(dm)\n",
    "    return dmfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:50:47.945336Z",
     "start_time": "2023-10-06T02:50:47.842573Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dmfccs = compute_delta_mfccs(train_mfccs)\n",
    "test_dmfccs  = compute_delta_mfccs(test_mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:50:47.954685Z",
     "start_time": "2023-10-06T02:50:47.950231Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_dmfccs[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing - bag-of-words\n",
    "\n",
    "The next problem you might notice is that the number of MFCCs is different for each sound, since sound can have different lengths.  Hence, before using our machine learning algorithms, we need to encode the MFCCs into a vector.\n",
    "\n",
    "One solution is to use a \"bag-of-audio-words\" representation, which is analogous to the bag-of-words representation for text.\n",
    "Here, we build a vocabulary of \"audio-words\" and map each MFCC to one of the words.  Then we can represent each sound as a histogram of counts.\n",
    "\n",
    "We will use the k-means clustering algorithm to build the codebook of audio words.  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:50:48.040138Z",
     "start_time": "2023-10-06T02:50:47.956337Z"
    }
   },
   "outputs": [],
   "source": [
    "# put dmfccs from all training data together\n",
    "all_dmfccs = vstack(train_dmfccs)\n",
    "print(all_dmfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:50:57.340302Z",
     "start_time": "2023-10-06T02:50:48.041444Z"
    }
   },
   "outputs": [],
   "source": [
    "# run k-means to build codebook\n",
    "km = cluster.KMeans(n_clusters=100, random_state=4487)\n",
    "km.fit(all_dmfccs[0::100])  # subsample by 10 to make it faster\n",
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform the data into BOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:50:57.345216Z",
     "start_time": "2023-10-06T02:50:57.342033Z"
    }
   },
   "outputs": [],
   "source": [
    "def bow_transform(model, mfccs):\n",
    "    numwords = model.cluster_centers_.shape[0]\n",
    "    bows = zeros((len(mfccs), numwords))\n",
    "    for i in range(len(mfccs)):\n",
    "        w = model.predict(mfccs[i])\n",
    "        bw = bincount(w, minlength=numwords)\n",
    "        bows[i,:] = bw\n",
    "    return bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:15.272945Z",
     "start_time": "2023-10-06T02:50:57.348028Z"
    }
   },
   "outputs": [],
   "source": [
    "train_bow = bow_transform(km, train_dmfccs)\n",
    "test_bow  = bow_transform(km, test_dmfccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag pre-processing\n",
    "\n",
    "Next, we extract all the tags from the data, and get a unique list of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:15.282844Z",
     "start_time": "2023-10-06T02:51:15.274086Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alltagnames, alltagnames_counts = unique(concatenate(train_tags), return_counts=True)\n",
    "for a,b in zip(alltagnames, alltagnames_counts):\n",
    "    print(\"{}: {}\".format(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove some tags that are not sufficiently present in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:15.286316Z",
     "start_time": "2023-10-06T02:51:15.283936Z"
    }
   },
   "outputs": [],
   "source": [
    "blacklist = ['2-3_hoe-ram_presence', '2-4_pile-driver_presence', '5-2_car-alarm_presence', '5-X_other-unknown-alert-signal_presence', '6-3_ice-cream-truck_presence', '7-3_large-crowd_presence', '7-X_other-unknown-human-voice_presence']\n",
    "tagnames = [t for t in alltagnames if t not in blacklist]\n",
    "print(tagnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the list of tags for each sound into binary attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:15.290128Z",
     "start_time": "2023-10-06T02:51:15.287893Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert list of tags into binary class labels\n",
    "def tags2class(tags, tagnames):\n",
    "    b = zeros(shape=(len(tags), len(tagnames)))\n",
    "    for i,t in enumerate(tags):\n",
    "        for j,n in enumerate(tagnames):\n",
    "            if n in t:\n",
    "                b[i,j] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:15.299022Z",
     "start_time": "2023-10-06T02:51:15.291116Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_classes[i,j] = absence/presence of the j-th tag in the i-th sound\n",
    "train_classes = tags2class(train_tags, tagnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:15.304255Z",
     "start_time": "2023-10-06T02:51:15.300137Z"
    }
   },
   "outputs": [],
   "source": [
    "# double check we did this correctly...\n",
    "# it should be the same as the tag counts above\n",
    "sum(train_classes,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline experiments\n",
    "\n",
    "Next, we will run a baseline experiment doing semantic tagging with bag-of-audio words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply TF-IDF to the count vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:15.315911Z",
     "start_time": "2023-10-06T02:51:15.305679Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert to TF\n",
    "tf_trans = feature_extraction.text.TfidfTransformer(use_idf=True, norm='l1')\n",
    "train_Xtf = tf_trans.fit_transform(train_bow)\n",
    "test_Xtf  = tf_trans.transform(test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now learn a logisic regression classifier for each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:43.464785Z",
     "start_time": "2023-10-06T02:51:15.317825Z"
    }
   },
   "outputs": [],
   "source": [
    "tagmodels = {}\n",
    "for i,t in enumerate(tagnames):\n",
    "    print('training {} - {}'.format(i, t))\n",
    "    myY = train_classes[:,i].ravel()\n",
    "    lr = linear_model.LogisticRegressionCV(Cs=logspace(-4,4,20), cv=5, class_weight='balanced', solver='liblinear')\n",
    "    lr.fit(train_Xtf, myY)\n",
    "    tagmodels[t] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how we did on tagging the training set, we compute the tag scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:43.493252Z",
     "start_time": "2023-10-06T02:51:43.468859Z"
    }
   },
   "outputs": [],
   "source": [
    "train_predscore = zeros(shape=(len(train_mfccs), len(tagnames)))\n",
    "\n",
    "for i,t in enumerate(tagnames):\n",
    "    print('predicting {} - {}'.format(i, t))\n",
    "\n",
    "    tmp = tagmodels[t].decision_function(train_Xtf)\n",
    "    train_predscore[:,i] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then plot the ROC curve using the training classes and training predicted scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:43.499567Z",
     "start_time": "2023-10-06T02:51:43.494693Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot an ROC curve using class labels and class scores\n",
    "def plot_roc(tagnames, Yclasses, Yscores):\n",
    "    fprall = []\n",
    "    tprall = []\n",
    "    aucall = []\n",
    "    for i in range(len(tagnames)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(Yclasses[:,i], Yscores[:,i])\n",
    "        plt.plot(fpr, tpr, lw=0.5, alpha=0.5)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        fprall.append(fpr)\n",
    "        tprall.append(tpr)\n",
    "        aucall.append(auc)\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    all_fpr = unique(concatenate(fprall))\n",
    "    mean_tpr = zeros_like(all_fpr)\n",
    "    for i in range(len(tagnames)):\n",
    "        mean_tpr += interp(all_fpr, fprall[i], tprall[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= len(tagnames)\n",
    "\n",
    "    # auc of the average ROC curve\n",
    "    auc = metrics.auc(all_fpr, mean_tpr)\n",
    "\n",
    "    # average AUC\n",
    "    mc_auc = mean(aucall)\n",
    "\n",
    "    plt.plot(all_fpr, mean_tpr, 'k-', lw=2)\n",
    "    plt.title('MCAUC={:.4f}, AUC={:.4f}'.format(mc_auc, auc))\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:43.616662Z",
     "start_time": "2023-10-06T02:51:43.501137Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_roc(tagnames, train_classes, train_predscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that AUC is the AUC of the black curve, while MCAUC is the average of the AUCs for all the color curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the LR classifiers to the test set to predict the score for each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:43.632709Z",
     "start_time": "2023-10-06T02:51:43.619756Z"
    }
   },
   "outputs": [],
   "source": [
    "test_predscore = zeros(shape=(len(test_mfccs), len(tagnames)))\n",
    "\n",
    "for i,t in enumerate(tagnames):\n",
    "    print('predicting {} - {}'.format(i, t))\n",
    "\n",
    "    tmp = tagmodels[t].decision_function(test_Xtf)\n",
    "    test_predscore[:,i] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the scores, now lets look at the predicted tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:43.637540Z",
     "start_time": "2023-10-06T02:51:43.634960Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert binary class vector into a list of tags\n",
    "def class2tags(classes, tagnames):\n",
    "    tags = []\n",
    "    for n in range(classes.shape[0]):\n",
    "        tmp = []\n",
    "        for i in range(classes.shape[1]):\n",
    "            if classes[n,i]:\n",
    "                tmp.append(tagnames[i])\n",
    "        tags.append(\" \".join(tmp))\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the score into a binary class label using a threshold (usually 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:43.643232Z",
     "start_time": "2023-10-06T02:51:43.640715Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert score into binary class 0 or 1.  \n",
    "test_predclass = test_predscore>0\n",
    "\n",
    "# convert to tags\n",
    "test_predtags = class2tags(test_predclass, tagnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the predicted labels and the label scores. Positive label scores indicate presence of the sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:43.911723Z",
     "start_time": "2023-10-06T02:51:43.681469Z"
    }
   },
   "outputs": [],
   "source": [
    "ii = 0\n",
    "# view tags and audio\n",
    "print(test_predtags[ii])\n",
    "showAudio(test_info[ii])\n",
    "\n",
    "# view the scores\n",
    "plt.figure(figsize=(5,7))\n",
    "plt.barh(tagnames, test_predscore[ii]);\n",
    "plt.xlabel('label score')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write the tag scores for the test set for submission to Kaggle. We need to upload the **tag scores** (not the class predictions) so that Kaggle can generate the ROC curves and calculate AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:43.918563Z",
     "start_time": "2023-10-06T02:51:43.914579Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_csv_kaggle_tags(fname, tagnames, Yscores):\n",
    "    # header\n",
    "    tmp = [['Id']]\n",
    "    for t in tagnames:\n",
    "        tmp[0].append(t)    \n",
    "    \n",
    "    # add ID numbers for each Y, and usage if necessary\n",
    "    for i in range(len(Yscores)):\n",
    "        tmp2 = [(i+1)]\n",
    "        for t in range(len(tagnames)):\n",
    "            tmp2.append(Yscores[i,t])\n",
    "        \n",
    "        tmp.append(tmp2)\n",
    "        \n",
    "    # write CSV file\n",
    "    f = open(fname, 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(tmp)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T02:51:43.929713Z",
     "start_time": "2023-10-06T02:51:43.920856Z"
    }
   },
   "outputs": [],
   "source": [
    "write_csv_kaggle_tags(\"sound_bow_baseline.csv\", tagnames, test_predscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## YOUR CODE and DOCUMENTATION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
