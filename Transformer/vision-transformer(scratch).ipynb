{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "\n",
    "paper: https://arxiv.org/abs/2010.11929 \n",
    "\n",
    "reference: \n",
    " 1. https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    " 2. https://github.com/tintn/vision-transformer-from-scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Minist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, datapath=\"../CS5296/Spark/data\", partition=\"train\", transform=None):\n",
    "        datapath = Path(datapath).resolve()\n",
    "        if(partition==\"test\"):\n",
    "           self.dataframe = pd.read_csv(Path.joinpath(datapath,\"mnist_test.csv\"))\n",
    "        else:\n",
    "            self.dataframe = pd.read_csv(Path.joinpath(datapath,\"mnist_train.csv\"))\n",
    "        self.transform = transform\n",
    "        self.images = self.dataframe.drop('label', axis=1).values.astype(np.float32)\n",
    "        self.labels = self.dataframe['label'].values.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        # Convert the image to the PyTorch tensor format (H, W, C)\n",
    "        image = image.view().reshape(28,28,1)\n",
    "        label = label\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "train= MNISTDataset(transform=ToTensor())\n",
    "test = MNISTDataset(partition=\"test\",transform=ToTensor())\n",
    "x,y = train[0]\n",
    "print(x.shape,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 49, 16])\n"
     ]
    }
   ],
   "source": [
    "def patchify(images, n_patches):\n",
    "    n, c, h, w = images.shape\n",
    "\n",
    "    assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n",
    "    patch_size = h // n_patches\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n",
    "                patches[idx, i * n_patches + j] = patch.flatten()\n",
    "    return patches\n",
    "\n",
    "print(x.shape)\n",
    "print(patchify(x.reshape(1,1,28,28),7).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VIT](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*tA7xE2dQA_dfzA0Bub5TVw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,embed_dim:int=512, num_heads:int=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: dimension of embeding vector output\n",
    "            num_heads: number of self attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        # We ensure that the dimensions of the model is divisible by the number of heads\n",
    "        assert embed_dim % num_heads == 0, 'd_model is not divisible by h'\n",
    "\n",
    "        # Initialize dimensions\n",
    "        self.embed_dim = embed_dim # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.single_head_dims = embed_dim // num_heads # Dimension of each head's key, query, and value\n",
    "\n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim) # Query transformation shape:(512)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim) # Key transformation\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim) # Value transformation\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim) # Output transformation\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.embed_dim). Here, the attention scores are calculated by taking the dot product of queries (Q) and keys (K), and then scaling by the square root of the key dimension (embed_dim).\n",
    "\n",
    "        Args:\n",
    "           key : key matrix\n",
    "           query : query matrix\n",
    "           value : value matrix\n",
    "           mask: mask for decoder\n",
    "        Returns:\n",
    "           output vector from multihead attention \n",
    "        \"\"\"\n",
    "        d_k = Q.shape[-1] \n",
    "        attention_scores = torch.matmul(Q,K.transpose(-2,-1))/math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None: # mask is define\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1) # Applying softmax\n",
    "        attention_scores = torch.matmul(attention_scores,V)\n",
    "        return attention_scores\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        x= x.view(batch_size, seq_length, self.num_heads, self.single_head_dims)\n",
    "        return x.transpose(1, 2) # shape (batch_size, self.num_heads, seq_length, self.single_head_dims)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, single_head_dims = x.size() #(batch_size, self.num_heads, seq_length, self.single_head_dims)\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)\n",
    "\n",
    "    def forward(self,Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "    \n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model:int, d_ff:int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        d_model: Dimensionality of the model's input and output.\n",
    "        d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
    "        \"\"\"\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.GELU = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.GELU(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model:int, seq_length:int):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1) #[0,1,2,3,4,...]\n",
    "        # Creating the division term for the positional encoding formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        EvenIndexSize = int(d_model/2)\n",
    "        OddIndexSize = int(math.ceil((d_model-1)/2))\n",
    "        #print(EvenIndexSize,OddIndexSize)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)[:,0:EvenIndexSize] # start:End:Step\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:,0:OddIndexSize] # start:End:Step\n",
    "\n",
    "        # Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "        pe.unsqueeze(0) # add batch \n",
    "        # Registering 'pe' as buffer. Buffer is a tensor not considered as a model parameter\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch size , seq_length, dims)\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_ff, dropout, num_heads:int=8):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.layerNorm0 = nn.LayerNorm(d_ff)\n",
    "        self.attention = MultiHeadAttention(embed_dim=d_ff,num_heads=num_heads)\n",
    "        self.layerNorm1 = nn.LayerNorm(d_ff)        \n",
    "        self.feed_forward = PositionWiseFeedForward(d_model=d_ff,d_ff=d_ff*4)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: (batch size , seq_length, dims)\n",
    "        mask: (seq,length)\n",
    "        \"\"\"\n",
    "        x0=x\n",
    "        x = self.layerNorm0(x)\n",
    "        attn_out = self.attention(Q=x,K=x,V=x,mask=mask)\n",
    "        x= x0+self.dropout(attn_out)\n",
    "\n",
    "        x=self.layerNorm1(x)\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x= x+self.dropout(ff_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "class VIT(nn.Module):\n",
    "    def __init__(self, num_layers, out_class, d_model:int=16, chw=(1, 28, 28), num_patches=7 , d_ff:int=128, num_heads:int=8, dropout:float=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chw : channel, height width of a image\n",
    "            n_patches: number of patches of images\n",
    "            d_model: The dimensionality of the model's embeddings.\n",
    "            num_heads: Number of attention heads in the multi-head attention mechanism.\n",
    "            num_layers: Number of layers for both the encoder and the decoder.\n",
    "            d_ff: Hidden dimemsion.\n",
    "            dropout: Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(VIT, self).__init__()\n",
    "\n",
    "        self.chw = chw # (C, H, W)\n",
    "        self.n_patches = num_patches\n",
    "\n",
    "        n_w = chw[1] % num_patches\n",
    "        n_v = chw[2] % num_patches\n",
    "        \n",
    "        assert n_w == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert n_v == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "\n",
    "        self.linearEmbedding = nn.Linear(in_features=d_model, out_features=d_ff,device=\"cuda\")\n",
    "        # 2) Learnable classifiation token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, d_ff))\n",
    "        #self.encoder_embedding = nn.Embedding(num_embeddings=src_vocab_size,embedding_dim=d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=d_ff, seq_length=(n_w * n_v)+1)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(num_heads=num_heads, d_ff=d_ff, dropout=dropout) for _ in range(num_layers)])\n",
    "\n",
    "        # 5) Classification MLPk\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_ff, out_class),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Inference step to generate predictions for the target sequence.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Input to the encoder.\n",
    "            tgt (torch.Tensor): Input to the decoder.\n",
    "\n",
    "        Returns:\n",
    "            output (list): List of predicted labels for the target sequence.\n",
    "        \"\"\"\n",
    "        #src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        n, c, h, w = x.shape\n",
    "        x = patchify(x, self.n_patches).to(x.get_device())\n",
    "        x = self.linearEmbedding(x) # linear embedding\n",
    "\n",
    "        # Create a learnable [CLS] token\n",
    "        # Similar to BERT, the [CLS] token is added to the beginning of the input sequence\n",
    "        # and is used to classify the entire sequence\n",
    "        x = torch.cat((self.class_token.expand(n, 1, -1), x), dim=1)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            x = enc_layer(x, None)\n",
    "        \n",
    "\n",
    "        x = x[:, 0]\n",
    "\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#example use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x=x.reshape([1,1,28,28]).to(device)\n",
    "\n",
    "\n",
    "model = VIT(1,10,16).to(device=device)\n",
    "res = model(x)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████| 938/938 [05:10<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Loss: 2.0602177639505754\n",
      "Test Accuracy of the model on the 10000 test images: 0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01: 100%|██████████| 938/938 [05:01<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] Loss: 1.9593385463076105\n",
      "Test Accuracy of the model on the 10000 test images: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02: 100%|██████████| 938/938 [05:00<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] Loss: 1.9283586875207896\n",
      "Test Accuracy of the model on the 10000 test images: 0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 03: 100%|██████████| 938/938 [04:55<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] Loss: 1.8953523865894977\n",
      "Test Accuracy of the model on the 10000 test images: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 04: 100%|██████████| 938/938 [05:09<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] Loss: 1.8802297072115737\n",
      "Test Accuracy of the model on the 10000 test images: 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 05:  27%|██▋       | 257/938 [01:23<04:06,  2.77it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "trainloader = DataLoader(train, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(test, batch_size=64, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VIT(2,10,16,d_ff=64).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "NUMS_EPOCHS = 10\n",
    "\n",
    "\n",
    "for epoch in range(NUMS_EPOCHS):\n",
    "    gc.collect()\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    batch_iterator = tqdm(trainloader, desc=f\"Processing Epoch {epoch+1:02d}\")\n",
    "    for batch in batch_iterator:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUMS_EPOCHS}] Avg Loss: {running_loss / len(trainloader)}\")\n",
    "\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, data in enumerate(testloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outs = model(inputs)\n",
    "                pred_y = torch.max(outs, 1)[1].data.squeeze()\n",
    "                accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "\n",
    "                \n",
    "    print(f'Test Accuracy of the model on the {len(testloader.dataset)} test images: %.2f' % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
