{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f790fa9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T07:18:03.042907Z",
     "iopub.status.busy": "2024-05-27T07:18:03.042636Z",
     "iopub.status.idle": "2024-05-27T07:18:09.524671Z",
     "shell.execute_reply": "2024-05-27T07:18:09.523568Z"
    },
    "papermill": {
     "duration": 6.492198,
     "end_time": "2024-05-27T07:18:09.527444",
     "exception": false,
     "start_time": "2024-05-27T07:18:03.035246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd \n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6d181",
   "metadata": {
    "papermill": {
     "duration": 0.006743,
     "end_time": "2024-05-27T07:18:09.542573",
     "exception": false,
     "start_time": "2024-05-27T07:18:09.535830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Multihead self-attention\n",
    "![Multihead image](https://i.imgur.com/JqJVrsj.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd76dfed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T07:18:09.555704Z",
     "iopub.status.busy": "2024-05-27T07:18:09.555282Z",
     "iopub.status.idle": "2024-05-27T07:18:09.987776Z",
     "shell.execute_reply": "2024-05-27T07:18:09.986787Z"
    },
    "papermill": {
     "duration": 0.441505,
     "end_time": "2024-05-27T07:18:09.989975",
     "exception": false,
     "start_time": "2024-05-27T07:18:09.548470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,embed_dim:int=512, num_heads:int=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: dimension of embeding vector output\n",
    "            num_heads: number of self attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        # We ensure that the dimensions of the model is divisible by the number of heads\n",
    "        assert embed_dim % num_heads == 0, 'd_model is not divisible by h'\n",
    "\n",
    "        # Initialize dimensions\n",
    "        self.embed_dim = embed_dim # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.single_head_dims = embed_dim // num_heads # Dimension of each head's key, query, and value\n",
    "\n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim) # Query transformation shape:(512)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim) # Key transformation\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim) # Value transformation\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim) # Output transformation\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.embed_dim). Here, the attention scores are calculated by taking the dot product of queries (Q) and keys (K), and then scaling by the square root of the key dimension (embed_dim).\n",
    "\n",
    "        Args:\n",
    "           key : key matrix\n",
    "           query : query matrix\n",
    "           value : value matrix\n",
    "           mask: mask for decoder\n",
    "        Returns:\n",
    "           output vector from multihead attention \n",
    "        \"\"\"\n",
    "        d_k = Q.shape[-1] \n",
    "        attention_scores = torch.matmul(Q,K.transpose(-2,-1))/math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None: # mask is define\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1) # Applying softmax\n",
    "        attention_scores = torch.matmul(attention_scores,V)\n",
    "        return attention_scores\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        x= x.view(batch_size, seq_length, self.num_heads, self.single_head_dims)\n",
    "        return x.transpose(1, 2) # shape (batch_size, self.num_heads, seq_length, self.single_head_dims)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, single_head_dims = x.size() #(batch_size, self.num_heads, seq_length, self.single_head_dims)\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)\n",
    "\n",
    "    def forward(self,Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "    \n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = torch.rand((2,4, 8)).to(device=device)  # 100 points in 20D (batch_size, seq_length, embed_dim)\n",
    "head= MultiHeadAttention(embed_dim=8, num_heads=2)\n",
    "head.to(device=device)\n",
    "res=head(Q=data,K=data,V=data)\n",
    "print(res.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63993f2",
   "metadata": {
    "papermill": {
     "duration": 0.005909,
     "end_time": "2024-05-27T07:18:10.002090",
     "exception": false,
     "start_time": "2024-05-27T07:18:09.996181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feed-Forward Network\n",
    "$$ FFN(X) = W_{2}max(0,W_{1}x+b_{1})+b_{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81038e38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T07:18:10.015204Z",
     "iopub.status.busy": "2024-05-27T07:18:10.014920Z",
     "iopub.status.idle": "2024-05-27T07:18:10.254214Z",
     "shell.execute_reply": "2024-05-27T07:18:10.253380Z"
    },
    "papermill": {
     "duration": 0.248349,
     "end_time": "2024-05-27T07:18:10.256470",
     "exception": false,
     "start_time": "2024-05-27T07:18:10.008121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.1662,  0.5394, -0.0850],\n",
      "        [-0.4150,  0.0663, -0.0619]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0362, -0.4348], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.3268, -0.7022],\n",
      "        [ 0.3915, -0.4777],\n",
      "        [-0.2185, -0.3007]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.2021, -0.1976,  0.0210], device='cuda:0', requires_grad=True)]\n",
      "torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model:int, d_ff:int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        d_model: Dimensionality of the model's input and output.\n",
    "        d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
    "        \"\"\"\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "pe=PositionWiseFeedForward(d_model=3,d_ff=2).to(device)\n",
    "print(list(pe.parameters()))\n",
    "data = torch.rand((2,5,3)).to(device=device)  # 100 points in 20D (batch_size, seq_length, embed_dim)\n",
    "res=pe(data)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765fddf3",
   "metadata": {
    "papermill": {
     "duration": 0.005934,
     "end_time": "2024-05-27T07:18:10.268678",
     "exception": false,
     "start_time": "2024-05-27T07:18:10.262744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Positional Encoding\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "Even indices(2i) :  PE(pos,2i) = sin(\\frac{pos}{1000^{\\frac{2i}{d-model}}})\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "Odd indices(2i+1) :  PE(pos,2i+1) = cos(\\frac{pos}{1000^{\\frac{2i}{d-model}}})\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "shape(pos,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2efc86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T07:18:10.282002Z",
     "iopub.status.busy": "2024-05-27T07:18:10.281738Z",
     "iopub.status.idle": "2024-05-27T07:18:10.367023Z",
     "shell.execute_reply": "2024-05-27T07:18:10.366019Z"
    },
    "papermill": {
     "duration": 0.09415,
     "end_time": "2024-05-27T07:18:10.368968",
     "exception": false,
     "start_time": "2024-05-27T07:18:10.274818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "torch.Size([2, 5, 784])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model:int, seq_length:int):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1) #[0,1,2,3,4,...]\n",
    "        # Creating the division term for the positional encoding formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        EvenIndexSize = int(d_model/2)\n",
    "        OddIndexSize = int(math.ceil((d_model-1)/2))\n",
    "        #print(EvenIndexSize,OddIndexSize)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)[:,0:EvenIndexSize] # start:End:Step\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:,0:OddIndexSize] # start:End:Step\n",
    "\n",
    "        # Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "        pe.unsqueeze(0) # add batch \n",
    "        # Registering 'pe' as buffer. Buffer is a tensor not considered as a model parameter\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch size , seq_length, dims)\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "pe=PositionalEncoding(d_model=784,seq_length=5).to(device)\n",
    "print(list(pe.parameters()))\n",
    "data = torch.rand((2,5,784)).to(device=device)  # 100 points in 20D (nums_seq, seq_length, embed_dim)\n",
    "res=pe(data)\n",
    "print(res.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739f9c73",
   "metadata": {
    "papermill": {
     "duration": 0.00602,
     "end_time": "2024-05-27T07:18:10.381135",
     "exception": false,
     "start_time": "2024-05-27T07:18:10.375115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Encoder Block\n",
    "\n",
    "![Encode block](https://images.datacamp.com/image/upload/v1691083306/Figure_2_The_Encoder_part_of_the_transformer_network_Source_image_from_the_original_paper_b0e3ac40fa.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a27fdbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T07:18:10.396189Z",
     "iopub.status.busy": "2024-05-27T07:18:10.395229Z",
     "iopub.status.idle": "2024-05-27T07:18:10.442051Z",
     "shell.execute_reply": "2024-05-27T07:18:10.441238Z"
    },
    "papermill": {
     "duration": 0.056788,
     "end_time": "2024-05-27T07:18:10.444443",
     "exception": false,
     "start_time": "2024-05-27T07:18:10.387655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_ff, dropout, d_model:int=512, num_heads:int=8):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim=d_model,num_heads=num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model=d_model,d_ff=d_ff)\n",
    "        self.layerNorm0 = nn.LayerNorm(d_model)\n",
    "        self.layerNorm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: (batch size , seq_length, dims)\n",
    "        mask: (seq,length)\n",
    "        \"\"\"\n",
    "        attn_out = self.attention.forward(Q=x,K=x,V=x,mask=mask)\n",
    "        x = self.layerNorm0( x + self.dropout(attn_out))\n",
    "        \n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.layerNorm1( x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "encoder=EncoderLayer(d_model=4,num_heads=2,d_ff=3,dropout=0.2).to(device)\n",
    "data = torch.rand((2,5,4)).to(device=device)  # 100 points in 20D (batch_size, seq_length, embed_dim)\n",
    "res=encoder.forward(data,mask=None)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe3e77",
   "metadata": {
    "papermill": {
     "duration": 0.006399,
     "end_time": "2024-05-27T07:18:10.457519",
     "exception": false,
     "start_time": "2024-05-27T07:18:10.451120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Decoder Block\n",
    "\n",
    "![Decoder block]( https://images.datacamp.com/image/upload/v1691083444/Figure_3_The_Decoder_part_of_the_Transformer_network_Souce_Image_from_the_original_paper_b90d9e7f66.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a88adeff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T07:18:10.472868Z",
     "iopub.status.busy": "2024-05-27T07:18:10.472534Z",
     "iopub.status.idle": "2024-05-27T07:18:10.497318Z",
     "shell.execute_reply": "2024-05-27T07:18:10.495993Z"
    },
    "papermill": {
     "duration": 0.034738,
     "end_time": "2024-05-27T07:18:10.499313",
     "exception": false,
     "start_time": "2024-05-27T07:18:10.464575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.maskedAttn =MultiHeadAttention(embed_dim=d_model,num_heads=num_heads)\n",
    "        self.layerNorm0 = nn.LayerNorm(d_model)\n",
    "        self.crossAttn = MultiHeadAttention(embed_dim=d_model,num_heads=num_heads)\n",
    "        self.layerNorm1 = nn.LayerNorm(d_model)\n",
    "        self.feedForward = PositionWiseFeedForward(d_model=d_model,d_ff=d_ff)\n",
    "        self.layerNorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self,x,encoder_output,src_mask,tgt_mask):\n",
    "        \n",
    "        attn_output = self.maskedAttn(x, x, x, tgt_mask)\n",
    "        x= self.layerNorm0(x + self.dropout(attn_output))\n",
    "        \n",
    "        cross_attn_output = self.crossAttn(x ,encoder_output, encoder_output, src_mask)\n",
    "        x= self.layerNorm1(x + self.dropout(cross_attn_output))\n",
    "        \n",
    "        ff_out = self.feedForward(x)\n",
    "        x= self.layerNorm2( x + self.dropout(ff_out))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "#example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "decoder=DecoderLayer(d_model=4,num_heads=2,d_ff=3,dropout=0.2).to(device)\n",
    "data = torch.rand((2,5,4)).to(device=device)  # 100 points in 20D (batch_size, seq_length, embed_dim)\n",
    "res=decoder.forward(x=data[:2,:-1,:4],encoder_output=data,src_mask=None,tgt_mask=None)\n",
    "print(res.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8798e36",
   "metadata": {
    "papermill": {
     "duration": 0.006378,
     "end_time": "2024-05-27T07:18:10.512708",
     "exception": false,
     "start_time": "2024-05-27T07:18:10.506330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transformer Model\n",
    "\n",
    "![transformer](https://images.datacamp.com/image/upload/v1691083566/Figure_4_The_Transformer_Network_Source_Image_from_the_original_paper_120e177956.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da166627",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T07:18:10.527105Z",
     "iopub.status.busy": "2024-05-27T07:18:10.526767Z",
     "iopub.status.idle": "2024-05-27T07:18:10.895492Z",
     "shell.execute_reply": "2024-05-27T07:18:10.894130Z"
    },
    "papermill": {
     "duration": 0.378645,
     "end_time": "2024-05-27T07:18:10.897915",
     "exception": false,
     "start_time": "2024-05-27T07:18:10.519270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 350]) torch.Size([8, 350])\n",
      "torch.Size([8, 350, 255])\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_vocab_size: Source vocabulary size (analogy input channels)\n",
    "            tgt_vocab_size: Target vocabulary size (analogy output channels)\n",
    "            d_model: The dimensionality of the model's embeddings.\n",
    "            num_heads: Number of attention heads in the multi-head attention mechanism.\n",
    "            num_layers: Number of layers for both the encoder and the decoder.\n",
    "            d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
    "            max_seq_length: Maximum sequence length for positional encoding.\n",
    "            dropout: Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(num_embeddings=src_vocab_size,embedding_dim=d_model)\n",
    "        self.decoder_embedding = nn.Embedding(num_embeddings=tgt_vocab_size,embedding_dim=d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=d_model, seq_length=max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model=d_model, num_heads=num_heads, d_ff=d_ff, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model=d_model, num_heads=num_heads, d_ff=d_ff, dropout=dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(in_features=d_model, out_features=tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        \"\"\"\n",
    "        create masks for the source and target sequences, ensuring that padding tokens are ignored and that future tokens are not visible during training for the target sequence.\n",
    "\n",
    "        \"\"\"\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=tgt_mask.get_device()), diagonal=1)).bool() # triu -> remove diagonal in symmertic matrix \n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def forward(self, src, tgt,src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Inference step to generate predictions for the target sequence.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Input to the encoder.\n",
    "            tgt (torch.Tensor): Input to the decoder.\n",
    "\n",
    "        Returns:\n",
    "            output (list): List of predicted labels for the target sequence.\n",
    "        \"\"\"\n",
    "        #src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "\n",
    "        src_embedded = self.encoder_embedding(src) # (nums_seq, seq_length) -> (nums_seq, seq_length, dims)\n",
    "        src_embedded = self.positional_encoding(src_embedded)\n",
    "        src_embedded = self.dropout(src_embedded)\n",
    "\n",
    "        tgt_embedded = self.decoder_embedding(tgt)\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded)\n",
    "        tgt_embedded  = self.dropout(tgt_embedded)\n",
    "        \n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n",
    "    \n",
    "#example usage\n",
    "\"\"\" \n",
    "# Generate random sample data\n",
    "src_data = torch.randint(low=1, high=src_vocab_size, size= (64, max_seq_length))  # (batch_size, seq_length) (64,256)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "model=Transformer(src_vocab_size=255, tgt_vocab_size=255,d_model=128,num_heads=2,num_layers=1,d_ff=3,max_seq_length=784,dropout=0.2).to(device)\n",
    "data = torch.randint(low=1, high=255, size= (8,350)).to(device=device)  # 100 points in 20D (num_seq, seq_length)\n",
    "src = data\n",
    "tgt =data[:,:]\n",
    "print(src.shape, tgt.shape)\n",
    "res=model(src=src,tgt=tgt)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ae729",
   "metadata": {
    "papermill": {
     "duration": 0.00733,
     "end_time": "2024-05-27T07:18:10.913072",
     "exception": false,
     "start_time": "2024-05-27T07:18:10.905742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e76213f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T07:18:10.929555Z",
     "iopub.status.busy": "2024-05-27T07:18:10.929180Z",
     "iopub.status.idle": "2024-05-27T07:18:12.554044Z",
     "shell.execute_reply": "2024-05-27T07:18:12.552800Z"
    },
    "papermill": {
     "duration": 1.635598,
     "end_time": "2024-05-27T07:18:12.556207",
     "exception": false,
     "start_time": "2024-05-27T07:18:10.920609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.680485725402832\n",
      "Epoch: 2, Loss: 8.536981582641602\n",
      "Validation Loss: 8.670391082763672\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "src_data = src_data.to(device=device)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = tgt_data.to(device=device)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:,1:])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "transformer.eval()\n",
    "\n",
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device=device)  # (batch_size, seq_length)\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device=device)  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff24c17",
   "metadata": {
    "papermill": {
     "duration": 0.006475,
     "end_time": "2024-05-27T07:18:12.570408",
     "exception": false,
     "start_time": "2024-05-27T07:18:12.563933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Opus dataset\n",
    "https://huggingface.co/datasets/Helsinki-NLP/opus-100/viewer/en-zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db0516de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T07:18:12.585152Z",
     "iopub.status.busy": "2024-05-27T07:18:12.584873Z",
     "iopub.status.idle": "2024-05-27T07:18:22.876729Z",
     "shell.execute_reply": "2024-05-27T07:18:22.875849Z"
    },
    "papermill": {
     "duration": 10.301929,
     "end_time": "2024-05-27T07:18:22.878956",
     "exception": false,
     "start_time": "2024-05-27T07:18:12.577027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 07:18:15.062993: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-27 07:18:15.063089: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-27 07:18:15.190070: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader,Dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64), #add padding \n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "    \n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang)) #config['tokenizer_file']=\"../tokeners/tokenizer{0}.json\"\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace() # splite by white space \n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else: # get tokenizer from the path\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "    \n",
    "def get_ds(config): # get dataset\n",
    "    # It only has the train split, so we divide it overselves\n",
    "    ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\",cache_dir=\"./data_cache\", split='train') #download dataset from hugging face\n",
    "\n",
    "    # Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caa973",
   "metadata": {
    "papermill": {
     "duration": 0.006798,
     "end_time": "2024-05-27T07:18:22.892911",
     "exception": false,
     "start_time": "2024-05-27T07:18:22.886113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74531a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T07:18:22.907613Z",
     "iopub.status.busy": "2024-05-27T07:18:22.907099Z",
     "iopub.status.idle": "2024-05-27T07:18:22.914950Z",
     "shell.execute_reply": "2024-05-27T07:18:22.914135Z"
    },
    "papermill": {
     "duration": 0.017306,
     "end_time": "2024-05-27T07:18:22.916867",
     "exception": false,
     "start_time": "2024-05-27T07:18:22.899561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 32,\n",
    "        \"num_epochs\": 25,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }\n",
    "\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "\n",
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65ec02de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T07:18:22.931676Z",
     "iopub.status.busy": "2024-05-27T07:18:22.931002Z",
     "iopub.status.idle": "2024-05-27T07:18:22.944214Z",
     "shell.execute_reply": "2024-05-27T07:18:22.943441Z"
    },
    "papermill": {
     "duration": 0.022561,
     "end_time": "2024-05-27T07:18:22.946096",
     "exception": false,
     "start_time": "2024-05-27T07:18:22.923535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Validation code\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    #encoder_output = model.encode(source, source_mask)\n",
    "    \n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        #out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "        out= model(src=source, tgt=decoder_input, src_mask=source_mask, tgt_mask=decoder_mask)\n",
    "        \n",
    "        # get next token\n",
    "        #print(f\"{out.shape} -> {out[:, -1].shape}\")\n",
    "        prob = torch.log_softmax(out[:, -1],dim = -1) # take last predict\n",
    "        \n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "    console_width = 80\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "                # check that the batch size is 1\n",
    "            assert encoder_input.size(\n",
    "                0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "\n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7ffd696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T07:18:22.960698Z",
     "iopub.status.busy": "2024-05-27T07:18:22.960404Z",
     "iopub.status.idle": "2024-05-27T11:27:22.250202Z",
     "shell.execute_reply": "2024-05-27T11:27:22.249142Z"
    },
    "papermill": {
     "duration": 14939.300308,
     "end_time": "2024-05-27T11:27:22.253043",
     "exception": false,
     "start_time": "2024-05-27T07:18:22.952735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549969cd8d3544d4b0e5a731f316ec45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 5.73M/5.73M [00:00<00:00, 20.5MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51af7ef42010431f84d8b2cff5d9219f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of source sentence: 309\n",
      "Max length of target sentence: 274\n",
      "No model to preload, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████| 910/910 [09:53<00:00,  1.53it/s, loss=5.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: There must be some misunderstanding!' said Oblonsky.\n",
      "    TARGET: Qui c’è un equivoco — egli disse.\n",
      " PREDICTED: — Non è mai — disse Stepan Arkad ’ ic .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I've been bargaining with him for wheat and offering a good price.'\n",
      "    TARGET: Stavo trattando per il frumento, offrivo dei bei soldi, io.\n",
      " PREDICTED: — Io non posso nulla di lui e lo posso un ’ altra .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=5.555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Once he dined there, and the other time he spent an evening with some visitors, but he had not once stayed the night, as he used to do in former years.\n",
      "    TARGET: Una volta vi aveva pranzato, un’altra volta aveva passato la serata con ospiti, ma non vi aveva neanche una volta passato la notte, come era solito fare gli anni precedenti.\n",
      " PREDICTED: Il giorno , e , e , con un ’ altra cosa , egli aveva fatto che non aveva fatto con un ’ altra cosa , ma egli aveva fatto in modo di non aveva fatto .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I now closed Morton school, taking care that the parting should not be barren on my side.\n",
      "    TARGET: Chiusi la scuola di Morton avendo cura che la separazione, almeno da parte mia, non riuscisse sterile.\n",
      " PREDICTED: Io ho detto che non ho detto che la mia vita non mi .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=5.439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'And is it true that his wife is here?'\n",
      "    TARGET: — Ma è vero che la Karenina è qui?\n",
      " PREDICTED: — E è vero che la moglie è la moglie ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was the third year that Oblonsky had been Head of that Government Board in Moscow, and he had won not only the affection but also the respect of his fellow-officials, subordinates, chiefs, and all who had anything to do with him.\n",
      "    TARGET: Occupando già da tre anni il posto di capo di uno degli uffici amministrativi di Mosca, Stepan Arkad’ic aveva conquistato, oltre la simpatia, la stima dei colleghi, dei dipendenti, dei superiori, e di tutti coloro che avevano a che fare con lui.\n",
      " PREDICTED: Era il vecchio principe che Stepan Arkad ’ ic era stato stato stato stato da Mosca , e non solo solo non solo , ma , per lui , per lui , aveva fatto tutto quello che gli aveva fatto tutto quello che gli aveva fatto tutto .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 03: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=5.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Yes we must go away.\n",
      "    TARGET: — Sì, bisogna partire.\n",
      " PREDICTED: — Sì , andiamo .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And all at once a strange sensation came over him.\n",
      "    TARGET: E a un tratto lo afferrò una strana sensazione.\n",
      " PREDICTED: E subito subito subito una volta , lo guardò .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 04: 100%|██████████| 910/910 [09:55<00:00,  1.53it/s, loss=4.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: You can whistle till you nearly burst your boiler before they will trouble themselves to hurry.\n",
      "    TARGET: Potete fischiare fino a far scoppiar la caldaia, prima che si scomodino a tirarsi da parte.\n",
      " PREDICTED: Voi dovete a voi , prima che prima di prima .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He hurried downstairs feeling that he must do something, he knew not what.\n",
      "    TARGET: Andò giù a passi svelti: sentiva di dover agire, ma non sapeva come.\n",
      " PREDICTED: Egli si avvicinò che cosa si qualcosa , non sapeva cosa .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 05: 100%|██████████| 910/910 [09:55<00:00,  1.53it/s, loss=4.843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Who are you?\" looking at me with surprise and a sort of alarm, but still not wildly.\n",
      "    TARGET: Poi mi fissò con uno sguardo meravigliato, sgomento, ma non smarrito.\n",
      " PREDICTED: — Chi siete ? — mi domandò con un ' espressione di un ' altra , ma non mi .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Call it what you like,' said the Cat. 'Do you play croquet with the Queen to-day?'\n",
      "    TARGET: — Di' come ti pare, — rispose il Gatto. — Vai oggi dalla Regina a giocare a croquet?\n",
      " PREDICTED: — E che cosa avete detto — disse il Gatto . — Vi prego di Regina con la Regina ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 06: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=4.623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'What are you talking about?' asked Oblonsky, entering the room from his study and addressing his wife.\n",
      "    TARGET: — Che c’è — chiese Stepan Arkad’ic, venendo fuori dallo studio e rivolgendosi alla moglie.\n",
      " PREDICTED: — Che cosa avete mai di parlare ? — chiese Stepan Arkad ’ ic , indicando la moglie e , la moglie .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Therefore, the duke erred in his choice, and it was the cause of his ultimate ruin.\n",
      "    TARGET: Errò, adunque, el duca in questa elezione; e fu cagione dell'ultima ruina sua.\n",
      " PREDICTED: E , adunque , nel suo stato , e fu la sua attenzione , fu la sua attenzione .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 07: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=4.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He was not feeling at all tired and was only longing to work again and to accomplish as much as he could.\n",
      "    TARGET: Non sentiva più nessuna stanchezza; voleva solo lavorare sempre più svelto e sempre di più.\n",
      " PREDICTED: Non era neppure in nessun modo che si era e non solo per , e non poteva non poté .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Why, he's jealous!' she thought. 'Oh dear!\n",
      "    TARGET: “Perché è geloso — ella pensava. — Dio mio! com’è simpatico e sciocco!\n",
      " PREDICTED: “ Perché è proprio così !” pensava . — Ah , cara !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 08: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=3.757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Two feet off, you dimly observe a half-dressed ruffian, waiting to kill you, and you are preparing for a life-and-death struggle with him, when it begins to dawn upon you that it's Jim.\n",
      "    TARGET: Un paio di metri lontano, scorgete oscuramente un brigante seminudo che aspetta per ammazzarvi, e vi preparate per una lotta a sangue, quando comincia a balenarvi in niente che sia l’amico Gianni.\n",
      " PREDICTED: Due piedi , a metà della metà della metà della metà della metà , vi e vi per un po ’ di morte , quando lo con la morte , vi che vi .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The horse was not yet ready, but feeling particularly energetic, physically strong and alert to meet what lay before him, so as not to lose a moment he did not wait for it but started off on foot, telling Kuzma to catch him up.\n",
      "    TARGET: Il cavallo non era ancora pronto, ma, sentendo in sé un particolare tendersi delle forze fisiche e dell’attenzione verso quello che bisognava fare, per non perdere neanche un minuto, senza aspettare il cavallo, uscì a piedi e ordinò a Kuz’ma di raggiungerlo.\n",
      " PREDICTED: Il cavallo non era ancora pronta , ma , in particolare , e , , si a quello che non voleva , non si , ma che non si avvicinava , si , si avvicinò alla gamba , e si mise a .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 09: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=3.632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It had lost half its tail, one of its ears, and a fairly appreciable proportion of its nose.\n",
      "    TARGET: Aveva perduto la coda, un orecchio, e una parte del naso.\n",
      " PREDICTED: Era il suo dolore , una di , e un di di .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But Serezha, though he heard his tutor's weak voice, paid no heed to it.\n",
      "    TARGET: Ma Serëza, pur avendo sentito la voce fiacca dell’istitutore, non vi fece attenzione.\n",
      " PREDICTED: Ma Serëza , pur senza sentire la voce di Serëza , non aveva nessuna voce di .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 10: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=4.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"The carrier, no doubt,\" I thought, and ran downstairs without inquiry.\n",
      "    TARGET: — Il vetturino, — pensai, e scesi subito.\n",
      " PREDICTED: — Il pensiero non è vero , — pensavo , — e senza riflettere .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Do you, sir, feel calm and happy?\"\n",
      "    TARGET: — Signore, vi sentite calmo e felice?\n",
      " PREDICTED: — Volete bene , signore ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 11: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=3.464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: However, his father did not make him repeat it, but went on to the lesson from the Old Testament.\n",
      "    TARGET: Ma il padre non lo costrinse a ripetere e passò alla lezione sull’Antico Testamento.\n",
      " PREDICTED: Ma il padre non lo disse , ma andò da .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: CHAPTER VIII\n",
      "    TARGET: VIII\n",
      " PREDICTED: VIII\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 12: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=3.671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Supposing you came to dinner to-day?\n",
      "    TARGET: Magari oggi a pranzo.\n",
      " PREDICTED: Mettiamo oggi oggi oggi ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Why, he's jealous!' she thought. 'Oh dear!\n",
      "    TARGET: “Perché è geloso — ella pensava. — Dio mio! com’è simpatico e sciocco!\n",
      " PREDICTED: “ Ma lui è un ’ altra cosa !” ella pensava . — Ah , Dio mio , Dio mio , Dio mio , che fa bene !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 13: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=3.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'What can I write?' she thought. 'What can I decide alone?\n",
      "    TARGET: «Che posso scrivere? — pensava. — Che posso decidere da sola?\n",
      " PREDICTED: “ Che cosa posso scrivere ?” pensava . — Che cosa posso mai io ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Of that he was firmly convinced, and had long been so – ever since he had begun painting it; yet the opinion of others, whoever they might be, seemed to him of great importance, and disturbed him to the depths of his soul.\n",
      "    TARGET: Questo lo sapeva fermamente e lo sapeva da gran tempo, da quando aveva cominciato a dipingerlo; ma i giudizi degli altri, quali che fossero, avevano tuttavia per lui un’importanza enorme e lo agitavano fino in fondo all’anima.\n",
      " PREDICTED: S ’ egli era fermamente convinto che , ed era stata mai stata da tanto tempo , egli era ancora convinto di aver avuto l ’ opinione pubblica , e gli sembrava di essere di questa conoscenza di un grande importanza , l ’ importanza dell ’ anima lo amava .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 14: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=3.405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And it was after long searching that I found out the carpenter’s chest, which was, indeed, a very useful prize to me, and much more valuable than a shipload of gold would have been at that time.\n",
      "    TARGET: Di fatto dopo lunghe ricerche trovai la cassa del carpentiere più preziosa all’uso mio in quel momento, che nol sarebbe stato un galeone carico d’oro.\n",
      " PREDICTED: E dopo che mi la pelle , trovai fuori dal petto , che da una specie di era una specie di , che mi avrebbe trovato un di , molto tempo di una specie di .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Mr. Rochester held the candle over him; I recognised in his pale and seemingly lifeless face--the stranger, Mason: I saw too that his linen on one side, and one arm, was almost soaked in blood.\n",
      "    TARGET: Il signor Rochester avvicinò la candela e in quella testa pallida e inanimata riconobbi il signor Mason. Vidi che gli asciugamani che gli coprivano un braccio e un fianco erano intrisi di sangue.\n",
      " PREDICTED: Il signor Rochester lo vidi dietro a quella candela , e mi misi a pronunziare il volto pallido e a Mason , vidi il signor Mason e vidi una sola volta sul suo sangue .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 15: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=2.900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: These he read in the ante-room, so as not to let them divert his attention later on.\n",
      "    TARGET: Levin, proprio lì in anticamera, per non dimenticarsene poi, le lesse.\n",
      " PREDICTED: Questi venne in anticamera , in salotto non la sua attenzione , e poi tornò in silenzio .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: On that side!' she said irritably to Varenka, who was not wrapping the plaid round her feet the right way.\n",
      "    TARGET: Dall’altra parte! — disse con stizza a Varen’ka che le avvolgeva le gambe nello scialle non precisamente come voleva lei.\n",
      " PREDICTED: che , fianco , non era al Varen ’ ka che , non era con gli zoccoli , si voltò a destra e destra .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 16: 100%|██████████| 910/910 [09:55<00:00,  1.53it/s, loss=3.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Karenin was going to mention the bill that had been brought him, but his voice shook and he paused.\n",
      "    TARGET: — Aleksej Aleksandrovic voleva dire del conto che gli avevano portato, ma la voce cominciò a tremare ed egli si fermò.\n",
      " PREDICTED: Aleksej Aleksandrovic voleva dire che il suo conto era stato portato la voce , ma egli si scosse e tacque .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: With noiseless steps she advanced toward the bedside, went round so that he need not turn his head, and at once grasping his enormous skeleton hand with her fresh young one, pressed it, and with that sympathetic, quiet animation which gives no offence and is natural only to women, she began to talk to him.\n",
      "    TARGET: A passi leggeri si avvicinò svelta al lettuccio del malato e, accostandosi in modo che egli non avesse da voltare il capo, prese subito nella sua mano fresca, giovane, lo scheletro enorme della mano di lui, la strinse e, con quella sommessa animazione compassionevole, ma non offensiva, propria solo delle donne, cominciò a parlare con lui.\n",
      " PREDICTED: Con la solita piccola costituzione verso il letto , andò a voltarsi verso il capo , e poi , , non si mise a ridere con la testa in testa , e con la testa , , e con un giovane , che , senza volere si , cominciò a parlare di non altro che ella cominciò a parlare .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 17: 100%|██████████| 910/910 [09:55<00:00,  1.53it/s, loss=3.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He lifted his hand and opened his eyelids; gazed blank, and with a straining effort, on the sky, and toward the amphitheatre of trees: one saw that all to him was void darkness.\n",
      "    TARGET: Si fermò, non sapendo da qual lato volgere, stese la mano, sollevò le palpebre, guardò intorno a sé e facendo uno sforzo diresse gli occhi verso gli alberi e il cielo.\n",
      " PREDICTED: Sollevò la mano e aprì il braccio , e , con uno sforzo per guardare il cielo , lo vide verso gli alberi , e vide che il cielo era buio .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Yes,' said Alice doubtfully: 'it means--to--make--anything--prettier.'\n",
      "    TARGET: — Sì, — rispose Alice, ma un po' incerta: — significa... rendere... qualche cosa... più bella.\n",
      " PREDICTED: — Sì , — disse Alice ; — per fare qualche cosa , fare il , tutto è vero .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 18: 100%|██████████| 910/910 [09:55<00:00,  1.53it/s, loss=3.196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And she was surprised that formerly this had seemed impossible to her, and laughingly explained to them how much simpler it really was, and that they were both now contented and happy.\n",
      "    TARGET: Ed ella stupiva come questo le fosse apparso prima impossibile, e spiegava loro, ridendo, che era molto più semplice, e che ora entrambi erano felici e contenti.\n",
      " PREDICTED: E si accorse che quest ’ ora non si fosse possibile a lei e che , per quanto fosse colpevole , si trattava di loro e di quanto si felice , e tutti e due erano felici .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: These eyes in the Evening Star you must have seen in a dream.\n",
      "    TARGET: gli occhi di questa \"Stella Vespertina\" avete dovuto vederli in sogno.\n",
      " PREDICTED: Questi occhi , se siete stata l ’ aria un sogno .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 19: 100%|██████████| 910/910 [09:55<00:00,  1.53it/s, loss=2.762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"It would be so much better,\" she said, \"if she could only get out of the way for a month or two, till all was over.\"\n",
      "    TARGET: \"Sarebbe meglio per me, — diceva, — che me ne andassi per un mese o due finché tutto non sarà finito.\"\n",
      " PREDICTED: — Sarebbe meglio , — disse , — se si fosse possibile soltanto due o tre giorni .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Mama!' When he reached her he clung round her neck.\n",
      "    TARGET: Giunto di corsa fino a lei, le si appese al collo.\n",
      " PREDICTED: Mamma alla fine , egli la prese a guardare intorno a sé .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 20: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=2.944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Koznyshev was talking to Dolly, jokingly assuring her that the custom of going away after the wedding was spreading because newly-married couples always felt rather uncomfortable.\n",
      "    TARGET: Sergej Ivanovic parlava con Dar’ja Dmitrievna, sostenendo per scherzo che l’usanza di partire dopo il matrimonio è diffusa perché gli sposi novelli si vergognano sempre un poco.\n",
      " PREDICTED: Sergej Ivanovic voleva parlare di Dar ’ ja Aleksandrovna , che le si l ’ abito un matrimonio , il matrimonio di quel matrimonio per il matrimonio .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: This afternoon, instead of dreaming of Deepden, I was wondering how a man who wished to do right could act so unjustly and unwisely as Charles the First sometimes did; and I thought what a pity it was that, with his integrity and conscientiousness, he could see no farther than the prerogatives of the crown.\n",
      "    TARGET: Invece di sognare il mio paese, ero meravigliata che un uomo che amava il bene potesse commettere tante ingiustizie e pazzie, come quel Carlo I. Pensavo che è triste con quella integrità e quella coscienza di non ammetter nulla all'infuori dell'autorità.\n",
      " PREDICTED: Il pomeriggio di un pomeriggio di , fui chiesto come io potevo un uomo di , e mi pareva che la voglia dovesse fare ogni momento che non facesse più , e mi parve un ' onda che la testa non potesse veder più la sua attenzione .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 21: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=2.602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The soldiers were silent, and looked at Alice, as the question was evidently meant for her.\n",
      "    TARGET: I soldati tacevano e guardavano Alice, pensando che la domanda fosse rivolta a lei.\n",
      " PREDICTED: La soldati tacque , e Alice guardò ansiosamente come se la domanda evidentemente .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: All that evening Dolly maintained her usual slightly bantering manner toward her husband, and Oblonsky was contented and cheerful, but not to the extent of seeming to forget his guilt after having obtained forgiveness.\n",
      "    TARGET: Tutta la serata Dolly fu, come al solito, leggermente canzonatoria col marito, e Stepan Arkad’ic contento e allegro, ma non tanto da apparire, dopo il perdono, dimentico della propria colpa.\n",
      " PREDICTED: Tutta quella sera , Dolly le sue forze verso la sua abituale , verso il tono canzonatorio , Stepan Arkad ’ ic era allegro e allegro che non e non cessava di , ma dopo il suo perdono .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 22: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=2.589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The landowner looked at him.\n",
      "    TARGET: Il proprietario lo guardò.\n",
      " PREDICTED: Il proprietario gli guardò .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Even down to small details Koznyshev found in her all that he desired in a wife: she was poor and solitary, so that she would not bring into her husband's house a crowd of relations and their influence, as he saw Kitty doing. She would be indebted to her husband for everything, which was a thing he had always desired in his future family life.\n",
      "    TARGET: Perfino nei particolari Sergej Ivanovic trovava in lei tutto quello che avrebbe desiderato in una moglie: era povera e sola, sicché non avrebbe portato con sé un nugolo di parenti e la loro influenza in casa del marito, come egli vedeva nel caso di Kitty; ma avrebbe dovuto tutto al marito, cosa ch’egli aveva sempre desiderato per la propria futura vita familiare.\n",
      " PREDICTED: E , per andare a Sergej Ivanovic , trovava tutto quello che aveva , nei momenti penosi , e nella mente era così poco : nel marito , non avrebbe parlato a una casa , e i rapporti con i parenti e i parenti della signora Stahl , vedeva una cosa che , invece , era sempre , tuttavia nelle sue supposizioni della sua vita , sempre qualcosa di contrario .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 23: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=2.687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'You have only taken an idea from others, and distorted it, and you wish to apply it where it is inapplicable.'\n",
      "    TARGET: — Tu hai preso soltanto un’idea non tua, poi l’hai fatta diventar mostruosa e vuoi applicarla all’inapplicabile.\n",
      " PREDICTED: — Tu hai ricevuto un ’ idea dall ’ idea e gli altri lo desideri e desideri , e vuoi .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He was really dissatisfied, not because they had spent so much but because he had been reminded of a matter which, well knowing that something was wrong, he wished to forget.\n",
      "    TARGET: Realmente egli era contrariato, non che se ne andassero molti denari, ma che gli si ricordasse quello che lui, sapendo che v’era qualcosa che non andava, voleva dimenticare.\n",
      " PREDICTED: Era davvero disteso , non perché avevano passato tanto bene quanto era stato d ’ animo ; e sapendo che qualche cosa di sapere bene non voleva bene a nulla .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 24: 100%|██████████| 910/910 [09:54<00:00,  1.53it/s, loss=2.881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Now, what do you think?\n",
      "    TARGET: Dimmi, come credi tu?\n",
      " PREDICTED: Ma , adesso , cosa pensi ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The order was given: 'Mount!'\n",
      "    TARGET: Poi si udì: «in sella!».\n",
      " PREDICTED: L ' ordine era data .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "\n",
    "#training \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if (device == 'cuda'):\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "    print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "\n",
    "    \n",
    "Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)    \n",
    "model=Transformer(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size(),d_model=config['d_model'],num_heads=8,num_layers=6,d_ff=512,max_seq_length=config['seq_len'],dropout=0.15).to(device)\n",
    "\n",
    "writer = SummaryWriter(config['experiment_name'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "# If the user specified a model to preload before training, load it\n",
    "initial_epoch = 0\n",
    "global_step = 0\n",
    "preload = config['preload']\n",
    "model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "if model_filename:\n",
    "    print(f'Preloading model {model_filename}')\n",
    "    state = torch.load(model_filename)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    initial_epoch = state['epoch'] + 1\n",
    "    optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "    global_step = state['global_step']\n",
    "else:\n",
    "    print('No model to preload, starting from scratch')\n",
    "    \n",
    "\n",
    "for epoch in range(initial_epoch, config['num_epochs']):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "    for batch in batch_iterator:\n",
    "\n",
    "        encoder_input = batch['encoder_input'].to(device) # (B, seq_len)\n",
    "        decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "        encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "        decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "        label = batch['label'].to(device) # (B, seq_len)\n",
    "        # Run the tensors through the encoder, decoder and the projection layer\n",
    "        proj_output= model.forward(src=encoder_input,tgt=decoder_input,src_mask=encoder_mask,tgt_mask=decoder_mask)\n",
    "        #encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "        #decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "        #proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "        # Compare the output with the label\n",
    "       \n",
    "        #print(f\"label: {label.shape}: res: {proj_output.shape}\")\n",
    "        # Compute the loss using a simple cross entropy\n",
    "        loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "        batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "        # Log the loss\n",
    "        writer.add_scalar('train loss', loss.item(), global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "    run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "    \n",
    "    # Saving model\n",
    "    model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "    # Writting current model state to the 'model_filename'\n",
    "    torch.save({\n",
    "        'epoch': epoch, # Current epoch\n",
    "        'model_state_dict': model.state_dict(),# Current model state\n",
    "        'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n",
    "        'global_step': global_step # Current global step \n",
    "    }, model_filename)\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14969.11916,
   "end_time": "2024-05-27T11:27:29.534278",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-27T07:18:00.415118",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0254137e66814a08a009d646a08f6089": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "051158e006194464b2d7683e02f50b9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "072ae0f8e35c44ef9279a7e038ed3cf8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0bf738a7500b4abdaef3c4cbf175b0dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0c69d20b39bb4706a7b01b3c226da98d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3b99e28da2474b45bfe7386eeac382b5",
       "max": 32332.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0bf738a7500b4abdaef3c4cbf175b0dc",
       "value": 32332.0
      }
     },
     "145dc143b64e487c9f277aa77679cbdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b073543aaffa42d49b4ba21a7ac1c86b",
       "placeholder": "​",
       "style": "IPY_MODEL_c66de0aaa36742678a1ab131781d6a83",
       "value": " 32332/32332 [00:00&lt;00:00, 308750.31 examples/s]"
      }
     },
     "1d84b55001af49eab80139b051ca1b7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3b99e28da2474b45bfe7386eeac382b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "42b19526bef549e0bd7f74871ee81e7f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "49b4503b2be546b18ee9488e2ebebff4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b24896b81b1f4be195921ae59a89143e",
       "placeholder": "​",
       "style": "IPY_MODEL_9b3b5ace91f54b2ca7efed86807a76de",
       "value": " 1/1 [00:00&lt;00:00, 111.83it/s]"
      }
     },
     "51af7ef42010431f84d8b2cff5d9219f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d47e23ffb8ab40c380650496df59ab60",
        "IPY_MODEL_0c69d20b39bb4706a7b01b3c226da98d",
        "IPY_MODEL_145dc143b64e487c9f277aa77679cbdd"
       ],
       "layout": "IPY_MODEL_42b19526bef549e0bd7f74871ee81e7f"
      }
     },
     "549969cd8d3544d4b0e5a731f316ec45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_aba6a7e2ebad4e43994d3a643449475d",
        "IPY_MODEL_d3e589df840042bc8baa5bba455c5bc1",
        "IPY_MODEL_fddfcc492011471e81a0f94b99a4ad46"
       ],
       "layout": "IPY_MODEL_fb57e7538fb541949fe22282f1149376"
      }
     },
     "60300ad2a1934a3c83ffa6d3be634b9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "71bc40b1ca7a46cc84274423ed4d1407": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_072ae0f8e35c44ef9279a7e038ed3cf8",
       "placeholder": "​",
       "style": "IPY_MODEL_f5d32790760e430abf645ee0cb693f75",
       "value": "Computing checksums: 100%"
      }
     },
     "76e5210202614c278d2cf2a4b998b4ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ee5fd61906264f59a6e337b97a2ffacb",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9bba6e79d42a4ab5af1d4b0632e0e140",
       "value": 1.0
      }
     },
     "7c8a87e360b443edb71e7f923aafe20c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9b3b5ace91f54b2ca7efed86807a76de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9bba6e79d42a4ab5af1d4b0632e0e140": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "aba6a7e2ebad4e43994d3a643449475d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1d84b55001af49eab80139b051ca1b7d",
       "placeholder": "​",
       "style": "IPY_MODEL_0254137e66814a08a009d646a08f6089",
       "value": "Downloading readme: 100%"
      }
     },
     "adf7676024fd4ab2bf6ce056d5317c8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_71bc40b1ca7a46cc84274423ed4d1407",
        "IPY_MODEL_76e5210202614c278d2cf2a4b998b4ff",
        "IPY_MODEL_49b4503b2be546b18ee9488e2ebebff4"
       ],
       "layout": "IPY_MODEL_d602facfbd69493dacc64572c47c5c7e"
      }
     },
     "b073543aaffa42d49b4ba21a7ac1c86b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b24896b81b1f4be195921ae59a89143e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c66de0aaa36742678a1ab131781d6a83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d3e589df840042bc8baa5bba455c5bc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f0092250826c4f4f8fc9fc2038c947aa",
       "max": 28064.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f0396b56081d42e3b1a5d2c87f4b1580",
       "value": 28064.0
      }
     },
     "d47e23ffb8ab40c380650496df59ab60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7c8a87e360b443edb71e7f923aafe20c",
       "placeholder": "​",
       "style": "IPY_MODEL_051158e006194464b2d7683e02f50b9c",
       "value": "Generating train split: 100%"
      }
     },
     "d4c7d52f10c146cc8c15b32133cf04e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d602facfbd69493dacc64572c47c5c7e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ee5fd61906264f59a6e337b97a2ffacb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f0092250826c4f4f8fc9fc2038c947aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f0396b56081d42e3b1a5d2c87f4b1580": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f5d32790760e430abf645ee0cb693f75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fb57e7538fb541949fe22282f1149376": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fddfcc492011471e81a0f94b99a4ad46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d4c7d52f10c146cc8c15b32133cf04e6",
       "placeholder": "​",
       "style": "IPY_MODEL_60300ad2a1934a3c83ffa6d3be634b9c",
       "value": " 28.1k/28.1k [00:00&lt;00:00, 2.06MB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
