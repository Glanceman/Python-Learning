{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct U-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,stride=1,padding=\"same\",bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=out_channels,out_channels=out_channels,kernel_size=3,stride=1,padding=\"same\",bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels:int=3,out_channels:int=1, features:list=[64,128,256,512]):\n",
    "        super().__init__()\n",
    "        self.downs=nn.ModuleList()\n",
    "        self.ups=nn.ModuleList()\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        # down Unet\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels=in_channels,out_channels=feature))\n",
    "            in_channels=feature\n",
    "\n",
    "        #up \n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(in_channels=feature*2, out_channels=feature, kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.ups.append(DoubleConv(in_channels=feature*2, out_channels=feature))\n",
    "        \n",
    "        #botton layer latent \n",
    "        self.bottomnet=DoubleConv(in_channels=features[-1],out_channels=features[-1]*2)\n",
    "\n",
    "        self.final_conv= nn.Conv2d(in_channels=features[0],out_channels=1,kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        skip_connections=[]\n",
    "        for down in self.downs:\n",
    "\n",
    "            x=down(x)\n",
    "            skip_connections.append(x)\n",
    "            x=self.pool(x)\n",
    "\n",
    "        x=self.bottomnet(x)\n",
    "        skip_connections=skip_connections[::-1]\n",
    "\n",
    "        for index in range(0,len(self.ups),2):\n",
    "\n",
    "            #up conv\n",
    "            x=self.ups[index](x)\n",
    "        \n",
    "            skip_connection = skip_connections[index//2]\n",
    "            if(x.shape != skip_connection.shape): # max pooling with underflow the original size -> cause the new upsampling size smaller than origin size \n",
    "                x= tf.resize(x,size=skip_connection.shape[2:])\n",
    "            x=torch.cat((skip_connection, x),dim=1)\n",
    "\n",
    "            #pass to double conv\n",
    "            x=self.ups[index+1](x)\n",
    "\n",
    " \n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 160, 160])\n",
      "torch.Size([3, 1, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "x=torch.rand((3,1,160,160))\n",
    "model = UNET(in_channels=1,out_channels=1)\n",
    "pred  = model(x)\n",
    "print(pred.shape)\n",
    "print(x.shape)\n",
    "assert pred.shape ==x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load [Dataset](https://www.kaggle.com/c/carvana-image-masking-challenge/data?select=train_masks.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class CarvanaDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask # x ,y \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import albumentations as A \n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(DEVICE)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "NUM_WORKERS = 0\n",
    "IMAGE_HEIGHT = 160  # 1280 originally\n",
    "IMAGE_WIDTH = 240  # 1918 originally\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = True\n",
    "TRAIN_IMG_DIR = \"dataset/train/\"\n",
    "TRAIN_MASK_DIR = \"dataset/train_masks/\"\n",
    "VAL_IMG_DIR = \"dataset/valid/\"\n",
    "VAL_MASK_DIR = \"dataset/valid_masks/\"\n",
    "\n",
    "\n",
    "def train(loader,model, optimizer, loss_func,scaler):\n",
    "    loop = tqdm(loader)\n",
    "    print(\"train\")\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_func(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "## utils\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "def get_loaders(train_dir, train_maskdir, val_dir, val_maskdir, batch_size, train_transform, val_transform, num_workers=1, pin_memory=True,):\n",
    "    train_ds = CarvanaDataset(image_dir=train_dir, mask_dir=train_maskdir, transform=train_transform )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size,num_workers=num_workers, pin_memory=pin_memory, shuffle=True)\n",
    "\n",
    "    val_ds = CarvanaDataset(image_dir=val_dir, mask_dir=val_maskdir, transform=val_transform,)\n",
    "\n",
    "    val_loader = DataLoader( val_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def check_accuracy(loader, model, device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    model.eval() # disable Batch Normalization, Dropout \n",
    "    print(loader)\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / (\n",
    "                (preds + y).sum() + 1e-8\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
    "    )\n",
    "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
    "    model.train() # enable Batch Normalization, Dropout\n",
    "\n",
    "def save_predictions_as_imgs(loader, model, folder=\"saved_images/\", device=\"cuda\"):\n",
    "    model.eval()\n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "\n",
    "        torchvision.utils.save_image(\n",
    "            preds, f\"{folder}/pred_{idx}.png\"\n",
    "        )\n",
    "\n",
    "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000002499603C850>\n",
      "Got 20390117/25804800 with acc 79.02\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/276 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 276/276 [05:22<00:00,  1.17s/it, loss=0.136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000002499603C850>\n",
      "Got 25535006/25804800 with acc 98.95\n",
      "Dice score: 0.9756826758384705\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_images//pred_0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Python\\Image Segementation\\main.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Python/Image%20Segementation/main.ipynb#X11sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m check_accuracy(val_loader, model, device\u001b[39m=\u001b[39mDEVICE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Python/Image%20Segementation/main.ipynb#X11sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39m# print some examples to a folder\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Python/Image%20Segementation/main.ipynb#X11sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m save_predictions_as_imgs(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Python/Image%20Segementation/main.ipynb#X11sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     val_loader, model, folder\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msaved_images/\u001b[39m\u001b[39m\"\u001b[39m, device\u001b[39m=\u001b[39mDEVICE\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Python/Image%20Segementation/main.ipynb#X11sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m )\n",
      "\u001b[1;32me:\\Python\\Image Segementation\\main.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Python/Image%20Segementation/main.ipynb#X11sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m         preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(model(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Python/Image%20Segementation/main.ipynb#X11sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m         preds \u001b[39m=\u001b[39m (preds \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/Python/Image%20Segementation/main.ipynb#X11sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     torchvision\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39msave_image(\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Python/Image%20Segementation/main.ipynb#X11sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m         preds, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfolder\u001b[39m}\u001b[39;00m\u001b[39m/pred_\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Python/Image%20Segementation/main.ipynb#X11sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     )\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Python/Image%20Segementation/main.ipynb#X11sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     torchvision\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39msave_image(y\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfolder\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/Python/Image%20Segementation/main.ipynb#X11sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torchvision\\utils.py:150\u001b[0m, in \u001b[0;36msave_image\u001b[1;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m ndarr \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mmul(\u001b[39m255\u001b[39m)\u001b[39m.\u001b[39madd_(\u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mclamp_(\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39muint8)\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    149\u001b[0m im \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(ndarr)\n\u001b[1;32m--> 150\u001b[0m im\u001b[39m.\u001b[39msave(fp, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mformat\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:2428\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2426\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mr+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2427\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2428\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2430\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2431\u001b[0m     save_handler(\u001b[39mself\u001b[39m, fp, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_images//pred_0.png'"
     ]
    }
   ],
   "source": [
    "train_transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Rotate(limit=35, p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.1),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    ")\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    ")\n",
    "\n",
    "model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_loader, val_loader = get_loaders(\n",
    "        TRAIN_IMG_DIR,\n",
    "        TRAIN_MASK_DIR,\n",
    "        VAL_IMG_DIR,\n",
    "        VAL_MASK_DIR,\n",
    "        BATCH_SIZE,\n",
    "        train_transform,\n",
    "        val_transforms,\n",
    "        NUM_WORKERS,\n",
    "        PIN_MEMORY,\n",
    ")\n",
    "\n",
    "if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n",
    "\n",
    "\n",
    "check_accuracy(val_loader, model, device=DEVICE)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train(train_loader, model, optimizer, loss_fn, scaler)\n",
    "\n",
    "        # save model\n",
    "    checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\":optimizer.state_dict(),\n",
    "    }\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "        # check accuracy\n",
    "    check_accuracy(val_loader, model, device=DEVICE)\n",
    "\n",
    "        # print some examples to a folder\n",
    "    save_predictions_as_imgs(\n",
    "        val_loader, model, folder=\"saved_images/\", device=DEVICE\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
